{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalando o scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/4b/585e8e111ffb01466c59281f34febb13ad1a95d7fb3919fd57c33fc732a5/Scrapy-1.7.3-py2.py3-none-any.whl (234kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 9.1MB/s \n",
      "\u001b[?25hCollecting service-identity (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
      "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
      "Collecting cssselect>=0.9 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyOpenSSL in /srv/conda/envs/notebook/lib/python3.7/site-packages (from scrapy) (19.0.0)\n",
      "Collecting Twisted>=13.1.0; python_version != \"3.4\" (from scrapy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/af/d15d3686e56c704796b23b67552f3ac78b84652793091a7d7ac8bf2887d7/Twisted-19.7.0-cp37-cp37m-manylinux1_x86_64.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 19.8MB/s \n",
      "\u001b[?25hCollecting parsel>=1.5 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/86/c8/fc5a2f9376066905dfcca334da2a25842aedfda142c0424722e7c497798b/parsel-1.5.2-py2.py3-none-any.whl\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/45/1ba17c50a0bb16bd950c9c2b92ec60d40c8ebda9f3371ae4230c437120b6/w3lib-1.21.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: lxml; python_version != \"3.4\" in /srv/conda/envs/notebook/lib/python3.7/site-packages (from scrapy) (4.4.1)\n",
      "Collecting queuelib (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from scrapy) (1.12.0)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from service-identity->scrapy) (19.1.0)\n",
      "Collecting pyasn1 (from service-identity->scrapy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/71/8f0d444e3a74e5640a3d5d967c1c6b015da9c655f35b2d308a55d907a517/pyasn1-0.4.7-py2.py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 17.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: cryptography in /srv/conda/envs/notebook/lib/python3.7/site-packages (from service-identity->scrapy) (2.7)\n",
      "Collecting pyasn1-modules (from service-identity->scrapy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 27.2MB/s \n",
      "\u001b[?25hCollecting zope.interface>=4.4.2 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/b5/5cbafbe09bbfc242503b0cd9bcf9dee32458b3ca166f6a63cb7b4f465b60/zope.interface-4.6.0-cp37-cp37m-manylinux1_x86_64.whl (167kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 25.3MB/s \n",
      "\u001b[?25hCollecting PyHamcrest>=1.9.0 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d5/d37fd731b7d0e91afcc84577edeccf4638b4f9b82f5ffe2f8b62e2ddc609/PyHamcrest-1.9.0-py2.py3-none-any.whl (52kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 30.0MB/s \n",
      "\u001b[?25hCollecting Automat>=0.3.0 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/86/14c16bb98a5a3542ed8fed5d74fb064a902de3bdd98d6584b34553353c45/Automat-0.7.0-py2.py3-none-any.whl\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
      "Collecting constantly>=15.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
      "Collecting incremental>=16.10.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from cryptography->service-identity->scrapy) (0.24.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from cryptography->service-identity->scrapy) (1.12.3)\n",
      "Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.7/site-packages (from zope.interface>=4.4.2->Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (41.0.1)\n",
      "Requirement already satisfied: idna>=2.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from hyperlink>=17.1.1->Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (2.8)\n",
      "Requirement already satisfied: pycparser in /srv/conda/envs/notebook/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography->service-identity->scrapy) (2.19)\n",
      "Building wheels for collected packages: PyDispatcher\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp37-none-any.whl size=11516 sha256=d7f30c71140ad54136e0c806ed2139532a7f08c71dff718a6dfff65423f89810\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
      "Successfully built PyDispatcher\n",
      "Installing collected packages: pyasn1, pyasn1-modules, service-identity, PyDispatcher, cssselect, zope.interface, PyHamcrest, Automat, hyperlink, constantly, incremental, Twisted, w3lib, parsel, queuelib, scrapy\n",
      "Successfully installed Automat-0.7.0 PyDispatcher-2.0.5 PyHamcrest-1.9.0 Twisted-19.7.0 constantly-15.1.0 cssselect-1.1.0 hyperlink-19.0.0 incremental-17.5.0 parsel-1.5.2 pyasn1-0.4.7 pyasn1-modules-0.2.7 queuelib-1.5.0 scrapy-1.7.3 service-identity-18.1.0 w3lib-1.21.0 zope.interface-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um projeto scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'helloscrapy', using template directory '/srv/conda/envs/notebook/lib/python3.7/site-packages/scrapy/templates/project', created in:\n",
      "    /home/jovyan/labs/helloscrapy\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd helloscrapy\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject helloscrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.scrapy.ipynb  helloscrapy  lab1.shell-commands  lab2.querying  readme.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helloscrapy  scrapy.cfg\n"
     ]
    }
   ],
   "source": [
    "!ls helloscrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  middlewares.py  __pycache__  spiders\n",
      "items.py     pipelines.py    settings.py\n"
     ]
    }
   ],
   "source": [
    "!ls helloscrapy/helloscrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  __pycache__\n"
     ]
    }
   ],
   "source": [
    "!ls helloscrapy/helloscrapy/spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como extrair informações do site abaixo?\n",
    "http://quotes.toscrape.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um novo Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd helloscrapy/helloscrapy/spiders && touch first_spider.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executando o spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-11 16:56:50 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: helloscrapy)\n",
      "2019-10-11 16:56:50 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21) - [GCC 7.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Linux-4.15.0-65-generic-x86_64-with-debian-buster-sid\n",
      "2019-10-11 16:56:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'helloscrapy', 'NEWSPIDER_MODULE': 'helloscrapy.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['helloscrapy.spiders']}\n",
      "2019-10-11 16:56:50 [scrapy.extensions.telnet] INFO: Telnet Password: 44708c35265259fa\n",
      "2019-10-11 16:56:50 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-10-11 16:56:50 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-10-11 16:56:50 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-10-11 16:56:50 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-10-11 16:56:50 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-10-11 16:56:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-10-11 16:56:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-10-11 16:56:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\n",
      "2019-10-11 16:56:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/> (referer: None)\n",
      "2019-10-11 16:56:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/>\n",
      "{'titletext': ['<title>Quotes to Scrape</title>']}\n",
      "2019-10-11 16:56:50 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-10-11 16:56:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 446,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 2701,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 0.248829,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 10, 11, 16, 56, 50, 446663),\n",
      " 'item_scraped_count': 1,\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 52551680,\n",
      " 'memusage/startup': 52551680,\n",
      " 'response_received_count': 2,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/404': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2019, 10, 11, 16, 56, 50, 197834)}\n",
      "2019-10-11 16:56:50 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!cd helloscrapy && scrapy crawl spiderone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquivo robots.txt\n",
    "https://www.amazon.com.br/robots.txt\n",
    "\n",
    "https://www.americanas.com.br/robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy shell, CSS Selector, Xpath, Inspect, SelectorGadget (Chrome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy shell\n",
    "Digitar no terminal da máquina\n",
    "``` bash \n",
    "scrapy shell \"http://quotes.toscrape.com/\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS Selector\n",
    "\n",
    "``` bash\n",
    "response.css('title')\n",
    "\n",
    "response.css('title').extract()\n",
    "\n",
    "response.css('title::text').extract()\n",
    "\n",
    "response.css('title::text')[0].extract() #Se a lista tiver vazia apresenta erro\n",
    "\n",
    "response.css('title::text').extract_first() #Se não é possível extrair nada do site, retorna NULL como valor (não apresenta erro)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect (Navegador)\n",
    "```bash\n",
    "response.css('span.text').extract()\n",
    "\n",
    "response.css('span.text::text').extract()\n",
    "\n",
    "response.css('span.text::text')[1].extract()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selector Gadget (Google Chrome)\n",
    "https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=pt-BR\n",
    "\n",
    "```bash\n",
    "Pegnado todos os autores com o selector gadget: .author\n",
    "\n",
    "response.css('.author::text').extract()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XPath\n",
    "Mais poderosa que os seletores CSS porque pode-se analisar o conteúdo das tags. Ex: selecionar o link que contem o texto \"Próxima Página\"\n",
    "\n",
    "```bash\n",
    "response.xpath('//title').extract()\n",
    "\n",
    "response.xpath('//title/text()').extract()\n",
    "\n",
    "response.xpath(\"//span[@class='text']/text()\").extract()\n",
    "\n",
    "response.xpath('//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"text\", \" \" ))]/text()').extract() #usando o Selector Gadget\n",
    "\n",
    "response.xpath(\"//span[@class='text']/text()\")[2].extract()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect + CSS + Xpath\n",
    "Inspecionando o botão \"Next\"\n",
    "\n",
    "Extraindo seu conteúdo utilizando os seletores CSS e XPath:\n",
    "```bash\n",
    "response.css(\"li.next a\").xpath(\"@href\").extract()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-11 19:20:59 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: scrapybot)\n",
      "2019-10-11 19:20:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21) - [GCC 7.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Linux-4.15.0-65-generic-x86_64-with-debian-buster-sid\n",
      "2019-10-11 19:20:59 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\n",
      "2019-10-11 19:20:59 [scrapy.extensions.telnet] INFO: Telnet Password: 9223f36f080bd319\n",
      "2019-10-11 19:20:59 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage']\n",
      "2019-10-11 19:20:59 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-10-11 19:20:59 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-10-11 19:20:59 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-10-11 19:20:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n",
      "2019-10-11 19:20:59 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-10-11 19:21:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/> (referer: None)\n",
      "['/page/2/']\n"
     ]
    }
   ],
   "source": [
    "!scrapy shell \"http://quotes.toscrape.com/\" -c 'response.css(\"li.next a\").xpath(\"@href\").extract()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como coletar todas os links desta página?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-11 19:24:37 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: scrapybot)\n",
      "2019-10-11 19:24:37 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21) - [GCC 7.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Linux-4.15.0-65-generic-x86_64-with-debian-buster-sid\n",
      "2019-10-11 19:24:37 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\n",
      "2019-10-11 19:24:37 [scrapy.extensions.telnet] INFO: Telnet Password: b695a44fd543d60c\n",
      "2019-10-11 19:24:37 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage']\n",
      "2019-10-11 19:24:37 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-10-11 19:24:37 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-10-11 19:24:37 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-10-11 19:24:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n",
      "2019-10-11 19:24:37 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-10-11 19:24:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/> (referer: None)\n",
      "['/', '/login', '/author/Albert-Einstein', '/tag/change/page/1/', '/tag/deep-thoughts/page/1/', '/tag/thinking/page/1/', '/tag/world/page/1/', '/author/J-K-Rowling', '/tag/abilities/page/1/', '/tag/choices/page/1/', '/author/Albert-Einstein', '/tag/inspirational/page/1/', '/tag/life/page/1/', '/tag/live/page/1/', '/tag/miracle/page/1/', '/tag/miracles/page/1/', '/author/Jane-Austen', '/tag/aliteracy/page/1/', '/tag/books/page/1/', '/tag/classic/page/1/', '/tag/humor/page/1/', '/author/Marilyn-Monroe', '/tag/be-yourself/page/1/', '/tag/inspirational/page/1/', '/author/Albert-Einstein', '/tag/adulthood/page/1/', '/tag/success/page/1/', '/tag/value/page/1/', '/author/Andre-Gide', '/tag/life/page/1/', '/tag/love/page/1/', '/author/Thomas-A-Edison', '/tag/edison/page/1/', '/tag/failure/page/1/', '/tag/inspirational/page/1/', '/tag/paraphrased/page/1/', '/author/Eleanor-Roosevelt', '/tag/misattributed-eleanor-roosevelt/page/1/', '/author/Steve-Martin', '/tag/humor/page/1/', '/tag/obvious/page/1/', '/tag/simile/page/1/', '/page/2/', '/tag/love/', '/tag/inspirational/', '/tag/life/', '/tag/humor/', '/tag/books/', '/tag/reading/', '/tag/friendship/', '/tag/friends/', '/tag/truth/', '/tag/simile/', 'https://www.goodreads.com/quotes', 'https://scrapinghub.com']\n"
     ]
    }
   ],
   "source": [
    "!scrapy shell \"http://quotes.toscrape.com/\" -c 'response.css(\"a\").xpath(\"@href\").extract()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo citações e autores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
